
H2-3
Muokataan ssh astuksia:
sudo nano /etc/ssh/sshd_config
"
Varmista, että kohtaa PasswordAuthentication ei ole kommentoitu #-merkillä ja arvo on yes
Varmista myös, että kohtaa PermitRootLogin ei ole kommentoitu ja aseta sen arvoksi no
Sallitaan ainoastaan käyttäjän ubuntu kirjautuminen paikallisesta verkosta lisäämällä tiedostoon rivi AllowUsers ubuntu@192.168.1.0/24
"

Uudelleen käynnistetään:
sudo service sshd restart
Testataan:
ssh ubuntu@ip-osoite

lisää ip osoite ja nimeä kone:
sudo nano /etc/hosts

kafka:
wget http://www.nic.funet.fi/pub/mirrors/apache.org/kafka/2.2.2/kafka_2.12-2.2.2.tgz
mv kafka_2.12-2.2.2 /var/lib/kafka
cd /var/lib/kafka
bin/kafka-server-start.sh config/server.properties

zookeeper:
apt-get update && apt install openjdk-11-jre-headless
bin/zookeeper-server-start.sh config/zookeeper-1.properties


kafka topic esimerkki:
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic ajoneuvot
bin/kafka-topics.sh --list --bootstrap-server localhost:9092
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic ajoneuvot
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ajoneuvot --from-beginning


H4
A
sudo cp config/server.properties config/server-1.properties
sudo cp config/server.properties config/server-2.properties
sudo cp config/server.properties config/server-3.properties

sudo bin/zookeeper-server-start.sh config/zookeeper.properties
sudo bin/kafka-server-start.sh config/server-1.properties
sudo bin/kafka-server-start.sh config/server-2.properties
sudo bin/kafka-server-start.sh config/server-3.properties

bin/kafka-topics.sh -create -zookeeper localhost:3000 -replication-factor 3 -partitions 1 -topic tietokoneet
bin/kafka-topics.sh --describe --zookeeper localhost:3000 -topic tietokoneet

bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --topic tietokoneet
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --from-beginning --topic tietokoneet --partition 0

bin/kafka-topics.sh -create -zookeeper localhost:3000 -replication-factor 3 -partitions 3 -topic fillarit
bin/kafka-topics.sh --describe --zookeeper localhost:3000 -topic fillarit

B
cp config/zookeeper.properties config/zookeeper-1.properties
cp config/zookeeper.properties config/zookeeper-2.properties
cp config/zookeeper.properties config/zookeeper-3.properties

mkdir -p data/zookeeper1
mkdir -p data/zookeeper2
mkdir -p data/zookeeper3

echo 1 > data/zookeeper1/myid
echo 2 > data/zookeeper2/myid
echo 3 > data/zookeeper3/myid

bin/zookeeper-server-start.sh config/zookeeper-1.properties
bin/zookeeper-server-start.sh config/zookeeper-2.properties
bin/zookeeper-server-start.sh config/zookeeper-3.properties

sudo bin/kafka-server-start.sh config/server-1.properties
sudo bin/kafka-server-start.sh config/server-2.properties
sudo bin/kafka-server-start.sh config/server-3.properties

bin/kafka-topics.sh -create -zookeeper localhost:3000,localhost:3001,localhost:3002 --replication-factor 3 -partitions 3 -topic failsafe

bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --topic failsafe
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 -topic failsafe --consumer-property group.id=group-red

C

var/lib/kafka/bin/start-cluster.sh
chmod 755 bin/start-cluster.sh

#!/bin/bash
/var/lib/kafka/bin/zookeeper-server-start.sh /var/lib/kafka/config/zookeeper-1.properties &
/var/lib/kafka/bin/zookeeper-server-start.sh /var/lib/kafka/config/zookeeper-2.properties &
/var/lib/kafka/bin/zookeeper-server-start.sh /var/lib/kafka/config/zookeeper-3.properties &
/var/lib/kafka/bin/kafka-server-start.sh /var/lib/kafka/config/server-1.properties &
/var/lib/kafka/bin/kafka-server-start.sh /var/lib/kafka/config/server-2.properties &
/var/lib/kafka/bin/kafka-server-start.sh /var/lib/kafka/config/server-3.properties &


bin/kafka-server-stop.sh
bin/zookeeper-server-stop.sh
bin/start-cluster.sh

netstat -an | grep LISTEN

5
bin/kafka-topics.sh --create --zookeeper localhost:3000 --replication-factor 3 --partitions 3 --topic streams-plaintext-input
bin/kafka-topics.sh --create --zookeeper localhost:3000 --replication-factor 3 --partitions 3 --topic streams-wordcount-output

data ja syöte:
wget http://hantt.pages.labranet.jamk.fi/bigdata/exercises/first_stream_document.txt
wget http://hantt.pages.labranet.jamk.fi/bigdata/exercises/second_stream_document.txt
cat first_stream_document.txt | bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --topic streams-plaintext-input

word count demo:
bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --topic streams-wordcount-output --from-beginning --formatter kafka.tools.DefaultMessageFormatter --property print.key=true \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

6

tar xvf spark-2.4.7-bin-hadoop2.7.tgz
sudo mv spark-2.4.7-bin-hadoop2.7 /usr/local/spark

nano ~/.bashrc
export PATH=$PATH:/usr/local/spark/bin
source ~/.bashrc

export SPARK_MASTER_HOST='192.168.1.19'
export SPARK_LOCAL_UP='192.168.1.19'
export JAVA_HOME='/usr/lib/jvm/java-8-openjdk-amd64'
export PYSPARK_PYTHON='/usr/bin/python3'

käynnistä master:
cd /usr/local/spark
./sbin/start-master.sh
käynnistä slave:
cd /usr/local/spark
./sbin/start-slave.sh spark://192.168.1.19:7077
tsekki:
jps

putty tunnelointi ja tsekki selaimella http://localhost:8080/


7
netstat -an | grep LISTEN
sudo bin/kafka-server-stop.sh
sudo bin/zookeeper-server-stop.sh
cd /var/lib/kafka

sudo nano config/server-3.properties

sudo bin/start-cluster.sh

python --version
sudo apt install python3
sudo apt install python3-pip
pip3 install confluent-kafka simplejson fake

wget http://hantt.pages.labranet.jamk.fi/bigdata/exercises/sensor-data.py

bin/kafka-topics.sh --create --bootstrap-server 192.168.1.15:9092,192.168.1.15:9093,192.168.1.15:9094 --replication-factor 3 --partitions 3 --topic sensordata

python3 sensor-data.py -k 192.168.1.15:9092,192.168.1.15:9093,192.168.1.15:9094 -t sensordata -r 1 -n 5

spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0 kafka-spark-demo.py 192.168.1.15:9092,192.168.1.15:9093,192.168.1.15:9094 sensordata

sensors = lines.map(lambda y : "Aikaleima: " + str(y['date']) + " | AnturiID: " + str(y['id']) + " | Lämpötila: " + str(y['main']['temperature'])+ " | Kosteus: " + str(y['main']['humidity'])+ " | Paikkatieto: " + str(y['coord']['lat'])+ ", " + str(y['coord']['lon']))

brokers, topic = sys.argv[1:]
kvs = KafkaUtils.createDirectStream(ssc, [topic],{"metadata.broker.list": brokers})
lines = kvs.map(lambda x: json.loads(x[1]))
filtered_lines = lines.filter(lambda y : (y['main']['temperature'] >= 10 and y['main']['temperature'] <= 25))
sensors = filtered_lines.map(lambda y : str(y['id']) + ", " + str(y['main']['temperature']))
sensors.foreachRDD(lambda x: x.foreach(lambda y: print(y)))
ssc.start()
ssc.awaitTermination()


H8

Asennetaan Cassandra:
Listätään repo sources.list tiedostoon:
sudo echo "deb https://downloads.apache.org/cassandra/debian 311x main" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

Lisätään repon avaintiedot (tarvittaessa sudo su):
sudo curl https://downloads.apache.org/cassandra/KEYS | sudo apt-key add -

sudo apt update
sudo apt install cassandra
sudo service cassandra start
Cassandran komentorivi käyttis (Ctrl+d sulkee)
cqlsh

Tietokanta anturidatalle:
create keyspace sensordatadb with replication = {'class':'SimpleStrategy', 'replication_factor':1};

use sensordatadb;

create table sensordata(sensor int, date text, event_time text, coord_lat float, coord_lon float, temp float, hum int, primary key ((sensor,date),event_time));

describe sensordata;

Tiedon tallentaminen tietokantaan:
sudo nano /etc/cassandra/cassandra.yaml
"
start_rpc: true
rpc_address: 0.0.0.0
broadcast_rpc_address: 192.168.1.24
listen_address: 192.168.1.24
seed_provider:
    - seeds: "192.168.1.24"
"

sudo service cassandra restart
Tarkasta, että Cassandra kuuntelee porttia 9042 osoitteessa 0.0.0.0:
netstat -an | grep LISTEN

spark-master:
ssh ubuntu@master
sudo nano /usr/local/spark/kafka-spark-demo.py
"
from pyspark_cassandra import CassandraSparkContext, saveToCassandra
from datetime import datetime
..
appConf = SparkConf().setAppName("sensorDataApp").set("spark.cassandra.connection.host", "192.168.1.24")
sc = CassandraSparkContext(conf=appConf)
..
rdd = filtered_lines.map(lambda x : {"sensor": x['id'], "date": datetime.now().strftime("%d.%m.%y"), "event_time": datetime.now().strftime("%H:%M:%S"), "coord_lat":x['coord']['lat'], "coord_lon":x['coord']['lon'], "hum":x['main']['humidity'], "temp":x['main']['temperature']})

rdd.foreachRDD(lambda z: z.saveToCassandra("sensordatadb","sensordata"))
..
ssc.start()
"
käynnistä kafka koneen klusteri:
ssh ubuntu@kafka
sudo /var/lib/kafka/bin/start-cluster.sh

käynnistä anturidata skripti kafkalla:
cd /var/lib/kafka
python3 sensor-data.py -k 192.168.1.15:9092,192.168.1.15:9093,192.168.1.15:9094 -t sensordata -r 1 -n 5

käynnistä anturidata skripti spark-masterilla:
cd /usr/local/spark
bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,anguenot:pyspark-cassandra:2.4.0 kafka-spark-demo.py 192.168.1.15:9092,192.168.1.15:9093,192.168.1.15:9094 sensordata

tarkastaminen cassandralla:
cqlsh
use sensordatadb
select * from sensordata


H9
Kassandra koneelle
Zeppelin asennus:
wget http://www.nic.funet.fi/pub/mirrors/apache.org/zeppelin/zeppelin-0.8.2/zeppelin-0.8.2-bin-all.tgz
tar xvf zeppelin-0.8.2-bin-all.tgz
sudo mv zeppelin-0.8.2-bin-all /var/lib/zeppelin

konfiguraatio tiedosto
cd /var/lib/zeppelin
sudo cp conf/zeppelin-env.sh.template conf/zeppelin-env.sh
sudo nano conf/zeppelin-env.sh
"
export ZEPPELIN_ADDR=192.168.1.24
"
sudo bin/zeppelin-daemon.sh start
netstat -an | grep LISTEN


oikeudet, porttiohjaukset ja tunnelointi.

cqlsh
use "sensordatadb";
select * from sensordata;


H10

Kassandra koneelle kotijuureen:
sudo apt update
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 68818C72E52529D4
echo "deb http://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list
sudo apt update
sudo apt install -y mongodb-org
sudo mkdir /etc/mongodata

sudo nano /etc/mongod.conf
"
bindIp: 192.168.1.24
"
sudo mongod --dbpath /etc/mongodata --config /etc/mongod.conf
netstat -an | grep LISTEN

Spark master koneelle:
sudo apt install python3-setuptools
git clone https://github.com/mongodb/mongo-hadoop.git /usr/local/spark/mongo-hadoop
cd /usr/local/spark/mongo-hadoop/spark/src/main/python
sudo python3 setup.py install

kaikille spark koneille:
sudo apt install python3-pip && pip3 install pymongo

master:
cd /usr/local/spark
(cp conf/spark-env.sh.template conf/spark-env.sh)
nano conf/spark-env.sh
"
export PYTHONPATH=$PYTHONPATH:'/usr/local/spark/mongo-hadoop/spark/src/main/python/'
"

wget -P /usr/local/spark/jars https://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/mongo-hadoop-spark/2.0.2/mongo-hadoop-spark-2.0.2.jar

sudo nano kafka-spark-mongodb-demo.py
"
rdd.foreachRDD(lambda z: z.saveToMongoDB('mongodb://192.168.1.24:27017/sensordatadb.sensordata'))
"

kafkan koneelta:
 python3 sensor-data.py -k 192.168.1.15:9092,192.168.1.15:9093,192.168.1.15:9094 -t sensordata -r 1 -n 5

spark-master koneelta:
spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,org.mongodb.spark:mongo-spark-connector_2.11:2.4.0,org.mongodb.mongo-hadoop:mongo-hadoop-core:1.3.1,org.mongodb:mongo-java-driver:3.1.0 --jars jars/mongo-hadoop-spark-2.0.2.jar kafka-spark-mongodb-demo.py 192.168.1.15:9092,192.168.1.15:9093,192.168.1.15:9094 sensordata

mongodb koneelta (kassandra):
mongo 192.168.1.24
show databases;
use sensordatadb;
show collections;
db.sensordata.find();

t1
db.sensordata.find({"sensor":9601});
t2
db.sensordata.find({"temp": {$gte: 20}},{"coord_lat":true,"coord_lon":true,"temp":true,"_id":false});
t3
db.sensordata.find({"hum": {$gte: 90, $lte:92}},{"sensor":true,"hum":true,"event_time":true,"_id":false});


T11
Mongo eli kassandra kone:
asennetaan nodejs
sudo apt install nodejs
node -v
sudo mkdir /var/web
sudo apt install npm
cd /var/web

sudo wget -P /var/web http://hantt.pages.labranet.jamk.fi/bigdata/exercises/server.js

sudo nano server.js
"
MongoClient.connect('mongodb://192.168.1.24:27017', { useNewUrlParser: true }, (err, client) => {
..
db.sensordata.aggregate([                     
                     { $group: { _id: "$sensor", "minTemp":{$min: "$temp"}, "maxTemp":{$max: "$temp"}, "avgTemp":{$avg: "$temp"}, "minHum":{$min: "$hum"}, "maxHum":{$max: "$hum"}, "avgHum":{$avg: "$hum"} } }
                   ])
..
"

sudo wget -P /var/web/views http://hantt.pages.labranet.jamk.fi/bigdata/exercises/index.ejs

..


käynnistä zookeeper ja brokerit kafkalla:

varmista, että sparkit päällä (master & slaves)

käynnitä mongodb kassandran koneella:
sudo mongod --dbpath /etc/mongodata --config /etc/mond.conf

käynnistä master-spark skripti:
/usr/local/spark$ spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,org.mongodb.spark:mongo-spark-connector_2.11:2.4.0,org.mongodb.mongo-hadoop:mongo-hadoop-core:1.3.1,org.mongodb:mongo-java-driver:3.1.0 --jars jars/mongo-hadoop-spark-2.0.2.jar kafka-spark-mongodb-demo.py 192.168.1.15:9092,192.168.1.15:9093,192.168.1.15:9094 sensordata

käynnistä kafkan python skripti:
 python3 sensor-data.py -k 192.168.1.15:9092,192.1.1.15:9093,192.168.1.15:9094 -t sensordata -r 1 -n 5

käynistä nodejs mongo eli kassandra koneella:
node server.js

testaa localhost:3000

127.0.0.1 localhost
192.168.1.15 kafka
192.168.1.19 master
192.168.1.6 slave1
192.168.1.17 slave2
192.168.1.33 slave3
192.168.1.24 storage (kassandra)